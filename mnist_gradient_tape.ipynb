{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM4k/gSSyjN/ImajZroDgbX"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "uUKdNMvDa1tY"
      },
      "outputs": [],
      "source": [
        "# import the necessary packages\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from tensorflow.keras.layers import Conv2D\n",
        "from tensorflow.keras.layers import MaxPooling2D\n",
        "from tensorflow.keras.layers import Activation\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.losses import categorical_crossentropy\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.datasets import mnist\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import time\n",
        "import sys"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model(width, height, depth, classes):\n",
        "\n",
        "\tinput_shape = (height, width, depth) # dimensions of the input images\n",
        "\tchannel_dim = -1 # channels dimension to be \"channels last\" ordering\n",
        "\n",
        "\tmodel = Sequential([\n",
        "\n",
        "    # Layer Set 1 - Input Layer:\n",
        "    # CONV => RELU => BN => POOL\n",
        "    # Conv - 16 filters with 3x3 kernal, produces 16 feature maps\n",
        "\t\tConv2D(16, (3, 3), padding=\"same\", input_shape=input_shape),\n",
        "\t\tActivation(\"relu\"), # introduce non-linearity\n",
        "\t\tBatchNormalization(axis=channel_dim), # converge faster\n",
        "\t\t# reduce spatial dims (hgt & width) of each feature map\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "\n",
        "    # Layer Set 2:\n",
        "    # (CONV => RELU => BN) * 2 => POOL\n",
        "    # Conv - 32 filters with 3x3 kernal\n",
        "\t\tConv2D(32, (3, 3), padding=\"same\"),\n",
        "\t\tActivation(\"relu\"),\n",
        "\t\tBatchNormalization(axis=channel_dim),\n",
        "\t\tConv2D(32, (3, 3), padding=\"same\"),\n",
        "\t\tActivation(\"relu\"),\n",
        "\t\tBatchNormalization(axis=channel_dim),\n",
        "\t\tMaxPooling2D(pool_size=(2, 2)),\n",
        "\n",
        "\t\t# Layer Set 3:\n",
        "    # (CONV => RELU => BN) * 3 => POOL\n",
        "    # Conv - 64 filters with 3x3 kernal\n",
        "\t\tConv2D(64, (3, 3), padding=\"same\"),\n",
        "\t\tActivation(\"relu\"),\n",
        "\t\tBatchNormalization(axis=channel_dim),\n",
        "\t\tConv2D(64, (3, 3), padding=\"same\"),\n",
        "\t\tActivation(\"relu\"),\n",
        "\t\tBatchNormalization(axis=channel_dim),\n",
        "\t\tConv2D(64, (3, 3), padding=\"same\"),\n",
        "\t\tActivation(\"relu\"),\n",
        "\t\tBatchNormalization(axis=channel_dim),\n",
        "\t\tMaxPooling2D(pool_size=(2, 2)),\n",
        "\n",
        "    # Layer Set 4:\n",
        "\t\t# Fully Connected layer => RELU layers\n",
        "\t\tFlatten(),\n",
        "\t\tDense(256), # 256 neurons\n",
        "\t\tActivation(\"relu\"),\n",
        "\t\tBatchNormalization(), # axis not specified bc 1-D\n",
        "\t\t# 50% dropout reduces overfitting, increasing model generalization\n",
        "    Dropout(0.5),\n",
        "\n",
        "    # Layer Set 5 - Output Layer:\n",
        "    # softmax classifier\n",
        "    # each neuron is probability score of respective class\n",
        "\t\tDense(classes),\n",
        "\t\tActivation(\"softmax\")\n",
        "\t])\n",
        "\treturn model"
      ],
      "metadata": {
        "id": "O_UlQ3EKcJjO"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def step(X, y):\n",
        "  '''\n",
        "  Custom training loop (step function) that\n",
        "  encapsulates forward and backward pass of\n",
        "  data (single step of gradient descent) using\n",
        "  GradientTape and updating model weights.\n",
        "  '''\n",
        "\n",
        "  # keeps track of all operations that happen on trainable\n",
        "  # variables onto a \"tape\" for automatic differentiation.\n",
        "  # trainable variables = weights and biases\n",
        "  with tf.GradientTape() as tape:\n",
        "    # make a prediction using the model\n",
        "    pred = model(X)\n",
        "\n",
        "    # calculate loss using a typical loss\n",
        "    # function for multi-class classification\n",
        "    loss = categorical_crossentropy(y, pred)\n",
        "\n",
        "  # **** Automatic Differentiation ****\n",
        "  # calculate gradients using tape & chain rule\n",
        "  # A list of gradients corresponding to\n",
        "  # each trainable variable is returned.\n",
        "  grads = tape.gradient(loss, model.trainable_variables)\n",
        "\n",
        "  # optimizer updates the model weights using calculated gradients.\n",
        "  # output of zip looks like this:\n",
        "  #   paired = [(grad1, var1), (grad2, var2) ... ]\n",
        "  opt.apply_gradients(zip(grads, model.trainable_variables))"
      ],
      "metadata": {
        "id": "uKiyKBGMriuC"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize Training Parameters:\n",
        "# num of epochs, batch size, learning rate\n",
        "EPOCHS = 25\n",
        "BATCH_SZ = 64 # num of samples used to update weights\n",
        "LRN_RATE = 1e-3 # controls the size of the steps"
      ],
      "metadata": {
        "id": "aCcsm5bXtK2l"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load the MNIST dataset\n",
        "print(\"[INFO] loading MNIST dataset...\")\n",
        "((trainX, trainY), (testX, testY)) = mnist.load_data()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p4W7zUIT2cBo",
        "outputId": "6fce501c-07d3-4a20-a271-2b04db259105"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] loading MNIST dataset...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# add a channel dimension at last axis\n",
        "# to every image in the dataset\n",
        "trainX = np.expand_dims(trainX, axis=-1)\n",
        "\n",
        "# normalize images to the range [0, 1]\n",
        "# by rescaling the pixel intensities\n",
        "testX = np.expand_dims(testX, axis=-1)\n",
        "trainX = trainX.astype(\"float32\") / 255.0\n",
        "testX = testX.astype(\"float32\") / 255.0\n",
        "\n",
        "# one-hot encode the labels to\n",
        "# allow the model to predict the\n",
        "# probability of each class independently\n",
        "trainY = to_categorical(trainY, 10)\n",
        "testY = to_categorical(testY, 10)"
      ],
      "metadata": {
        "id": "ZIhSY8VI2hzy"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# build model\n",
        "print(\"[INFO] creating model...\")\n",
        "model = build_model(28, 28, 1, 10)\n",
        "\n",
        "steps_per_epoch = len(trainX) // BATCH_SZ\n",
        "decay_steps = steps_per_epoch * EPOCHS\n",
        "\n",
        "# Exponential decay schedule\n",
        "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
        "    initial_learning_rate=LRN_RATE,\n",
        "    decay_steps=decay_steps,\n",
        "    decay_rate=0.9,\n",
        "    staircase=True\n",
        ")\n",
        "# initialize optimizer\n",
        "opt = Adam(learning_rate=lr_schedule)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eixFp7zy2xXM",
        "outputId": "533bf621-1058-43a5-fc73-0eb510248529"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] creating model...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# compute number of batch updates per epoch\n",
        "num_updates = int(trainX.shape[0] / BATCH_SZ)\n",
        "\n",
        "# loop over the number of epochs\n",
        "for epoch in range(0, EPOCHS):\n",
        "\n",
        "\t# log current epoch number\n",
        "\tprint(\"[INFO] starting epoch {}/{}...\".format(epoch + 1, EPOCHS), end=\"\")\n",
        "\tsys.stdout.flush()\n",
        "\tepochStart = time.time()\n",
        "\n",
        "  # loop over the data in batch size increments\n",
        "\tfor i in range(0, num_updates):\n",
        "\n",
        "\t\t# determine starting and ending\n",
        "    # slice indexes for current batch\n",
        "\t\tstart = i * BATCH_SZ\n",
        "\t\tend = start + BATCH_SZ\n",
        "\n",
        "\t\t# take a step - apply step function\n",
        "\t\tstep(trainX[start:end], trainY[start:end])\n",
        "\n",
        "  # log timing information for the epoch\n",
        "\tepochEnd = time.time()\n",
        "\telapsed = (epochEnd - epochStart) / 60.0\n",
        "\tprint(\"[INFO] took {:.4} minutes\".format(elapsed))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ypnudzY6INH",
        "outputId": "9a5012b2-03e2-4b6d-ff06-dcb04b28b73c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] starting epoch 1/25...[INFO] took 4.427 minutes\n",
            "[INFO] starting epoch 2/25...[INFO] took 4.441 minutes\n",
            "[INFO] starting epoch 3/25...[INFO] took 4.363 minutes\n",
            "[INFO] starting epoch 4/25...[INFO] took 4.362 minutes\n",
            "[INFO] starting epoch 5/25...[INFO] took 4.345 minutes\n",
            "[INFO] starting epoch 6/25...[INFO] took 4.289 minutes\n",
            "[INFO] starting epoch 7/25...[INFO] took 4.285 minutes\n",
            "[INFO] starting epoch 8/25...[INFO] took 4.288 minutes\n",
            "[INFO] starting epoch 9/25...[INFO] took 4.265 minutes\n",
            "[INFO] starting epoch 10/25...[INFO] took 4.254 minutes\n",
            "[INFO] starting epoch 11/25...[INFO] took 4.246 minutes\n",
            "[INFO] starting epoch 12/25...[INFO] took 4.267 minutes\n",
            "[INFO] starting epoch 13/25...[INFO] took 4.213 minutes\n",
            "[INFO] starting epoch 14/25...[INFO] took 4.273 minutes\n",
            "[INFO] starting epoch 15/25...[INFO] took 4.242 minutes\n",
            "[INFO] starting epoch 16/25...[INFO] took 4.266 minutes\n",
            "[INFO] starting epoch 17/25...[INFO] took 4.228 minutes\n",
            "[INFO] starting epoch 18/25...[INFO] took 4.3 minutes\n",
            "[INFO] starting epoch 19/25...[INFO] took 4.246 minutes\n",
            "[INFO] starting epoch 20/25...[INFO] took 4.295 minutes\n",
            "[INFO] starting epoch 21/25...[INFO] took 4.276 minutes\n",
            "[INFO] starting epoch 22/25...[INFO] took 4.26 minutes\n",
            "[INFO] starting epoch 23/25...[INFO] took 4.287 minutes\n",
            "[INFO] starting epoch 24/25...[INFO] took 4.202 minutes\n",
            "[INFO] starting epoch 25/25...[INFO] took 4.274 minutes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Need to compile the model to\n",
        "# calculate accuracy using Keras functions.\n",
        "model.compile(optimizer=opt, loss=categorical_crossentropy, metrics=[\"acc\"])\n",
        "\n",
        "# compute accuracy\n",
        "(loss, acc) = model.evaluate(testX, testY)\n",
        "print(\"[INFO] test loss: {:.4f}\".format(loss))\n",
        "print(\"[INFO] test accuracy: {:.4f}\".format(acc))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MaL1TgIH_qTp",
        "outputId": "dacab923-a7be-4a99-b4b7-fc8661b5a02b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 10s 25ms/step - loss: 0.0450 - acc: 0.9908\n",
            "[INFO] test loss: 0.0450\n",
            "[INFO] test accuracy: 0.9908\n"
          ]
        }
      ]
    }
  ]
}